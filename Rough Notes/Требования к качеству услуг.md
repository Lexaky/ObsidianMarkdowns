**Created at** [20-12-2025 10:18]
**Tags**: #Database 
# Требования к качеству услуг


#### Рассматриваемое знание
Наши требования:
1) Отказоустойчивость - сервер с БД вообще не отключался, работал непрерывно - метрика uptime сервиса = 100%;
		Есть **Error Budget** - измеряется в процентах. Мы допускаем, что EB может быть 5% - значит в месяц 1,5 дня может быть downtime.
2) Отсутствие неправильных ответов на запросы - можем задать accuracy метрику 
3) Метрика скорости ответов на запросы не ниже/не выше некоторого A - метрика скорости должна быть в процентах
		Есть Time Budget, который мы разрешаем сервису тратить, но желательно так не делать. Нужно, чтобы сервис отвечал в месяц на какое-то количество запросов. Запросы идут неравномерно (ночью спят, днём работают). У нас есть кривая распределения, где пик запросов находится днём. Время ответов должно быть такое, чтобы он успевал отвечать, не формируя очереди ответов.
Service Level Indicators (SLI) - всевозможные требования, которые можно придумать к уровню сервиса нашей системы. SLI может быть много, в зависимости от типа нагрузки сервиса. Бывают требования: ошибки 500 не более 20% от общего процента ошибок. Надёжность - общий отказ инфраструктуры не чаще чем 2 раза в год (например). Все эти параметры вероятностные.
Uptime 99,99% поддерживать сложно, т.к. оборудование выйдет из строя.

Этот документ с SLI называется Service level agreement (SLA) - это то, что либо нам предоставляет датацентр, согласно этому соглашению. В них могут быть расписаны свои параметры. Можно и самому выдвинуть свои требования к сервису, согласно его Tier'у, если сервис проходил оценку сервиса.

Должны следить за соблюдением этого документа, поэтому есть цели - Service Level Objectives (SLO) - это цели команды, которые поддерживают работоспособность (Dev ops'ы, sec ops'ы, системные администраторы и другие).

OSI имеет 7 уровней. С 1 по 3 уровень - сетевой администратор. С 4 по 7 - системный администратор. За 6 уровень всё-равно отвечает пользователь, на самом деле.

Dev Ops - developer operation specialist - специалист поддержки разработчиков - отвечает за инфраструктуру разработки. Если внутри предприятия есть облачное хранилище кода (верификация, развёртывания), то DevOps будет отвечать за развёртывание и хранение этого кода. DevOps отвечает за работу CI/CD, она может быть основана на github, gitlab.

Если проблема с тем, что интернет вообще не поднимается - сначала проверяется электричество, если его нет - энергетик, если оно есть, то сетевик.

Сейчас есть специалисты SecOps - защита инфраструктуры от несанкционированного воздействия. Отвечает также за организацию инфраструктуры наружу (какие сервисы можно выставлять наружу, какие нельзя). Какой уровень нагрузки request per second (RPS) выдерживает, а какой уже критический уровень.

Системный администратор защищает от DDos'а (высокой загрузки RPS из различных источников). 

CDN - content delivery network - распределённые дата центры. Это сеть, которая распределённая (не в одном ДЦ). Чем больше разброс ДЦ, тем лучше.
Современная скорость сетей большая, но есть объёмы данных (3V):
Variability вариативность
Volume объём
Velocity скорость
В Больших данных включаются не только текст и цифры, но и аудио, и медиа (всё, что занимает большие объёмы).
Google после покупки YouTube решала проблему долгой загрузки видео (потокового видео). Если время загрузки превышает время воспроизведения некоторого куска, пользователь должен ждать. А смысл любого онлайн-кинотеатра или видеосервиса в том, чтобы пользователь не ждал этого. Чтобы решить вопрос загрузки, была придумана система Google Global Cache - она размещается у провайдеров по всему миру в виде отдельных серверов (или серверных шкафов) и спонсируется самим Google - даёт деньги и оборудование. Смысл в том, чтобы трафик, который запрашивает гугл - проходил через систему GGC.
Если кто-то у провайдера запрашивает один и тот же контент (на малоизвестный контент уйдёт мало запросов, а на трендовый - много запросов, его не нужно подгружать с источника, он скорее всего уже будет подгружен на хранилище GGC). Причём есть вытесняющее и отталкивающее хранение - есть объём памяти, его сохранили на носитель. В случае вытесняющего хранения носитель будет содержать последний вариант самого рейтингового контента, а то, что не запрашивается - будет вытесняться и удаляться. Если никогда не запрашивался контент, то запрос будет к другому CDN, откуда и будет доставлен.
Для YT время доставки сложного объёмного контента видео стремится к времени доставки от провайдера до пользователя, а это быстро (только трендового контента)

После того, как GGC сформирован, - если мы не Google, нам эмулировать GGC дорого. Мы можем кеширующие сервера разослать только значимым провайдерам (к которым миллиарды человек подключены). Есть такие большие провайдеры, у которых много серверов, и между мировыми провайдерами проложены петабайтовые магистральные сети, которые позволяют обмениваться данными в режиме реального времени.
Если мы запрашиваем контент, которого на сервере моего провайдера нет, то:
Если ссылку видит один из серверов в замкнутой сети CDN, она будет доставлена. Мы сможем доставлять картинки очень быстро.

Чтобы обеспечивать быстродействие (помимо программ, БД), важно и средство доставки. Если у нас картинки статичные, то мы можем их размешать, арендовав объём данных в CDN, и разместив ссылки на CDN. Таким образом скорость доставки контента увеличиться в другую страну. В России блокируются определённые CDN Роскомнадзором, на них лучше не размещать контент.

На системе доставки контента не должно хранится критически важной информации. CDN не гарантирует доставку контента, только скорость. Время извлечения картинки из CDN в обход CDN может быть очень долгим.

Задача с желудями и белкой:
1) Бурундук пробегает всё поле, собираемые все жёлуди вносит в общее хранилище (Основная БД);
2) Периодический мониторинг состояния окружающей среды (выбирает большое дерево, смотрит, появились ли новые жёлуди). В принципе нет цели собрать всё в одно хранилище, но теперь знаем, что жёлуди все лежат у нас и чтобы поддерживать целостность жёлудей, если у другого дерева нападали жёлуди, можно его под то же дерево закопать (второе хранилище). Пусть будет всего 3 хранилища. 
		Мы знаем, что в 1-м хранится 15, во 2-м - 7, в 3-м - 4.
		Теперь бегая по полю можно докидывать в одно из ближайших хранилищ. Теоретически можно перетаскивать из одной БД в другое.
Про наше: у нас есть основная БД (master), а также есть дополнительные. По сути это шардированные БД. 
В каждом шарде хранятся определённые разделённые куски данных.

Когда между базами обмениваемся, мы обмениваемся пакетами данных, которыми базы обмениваются. Получается так называемый "Топик" - он может ходить от базе к базе (или от внешнего мира дополнятся данными), и пополнять разные БД. Топик хранит последние изменения с тех пор, как мы фиксировали в БД. Может несколько топиков бегать между базами.
Каждая база хранит внутри себя фиксированное состояние + последнее изменение, которое оно ещё не согласовала с другой структурой баз. 
Сумма внешних изменений - ...

3

Просто так отправлять куски транзакций и их согласовывать не получится. Нужно поддерживать стандарт BASE.
Пользователь видит одну БД, он обращается к полноценной БД. Вот эта "БД" - это материализованное представление всей этой дикой структуры (3)
4

Пользователь делает запрос на pull по образцу. Мы исходя из текущего состояния формируем ответ пользователю и возвращаем. 
Push запрос - materialized view (MV) может поменять своё текущее состояние, и в какой-то момент система меняет своё MV - в момент, например, расписания (чтобы пользователь не мешал своими запросами, мы можем вливать в БД банки??), либо надо отслеживать E - eventual consistency - если запросы пользователя направлены в места, которые ещё не согласованы с банками данных внутри, то они не вернутся?

Потоковый процессор // Apache Kafka - менеджер сообщений. А топики в AK - это и есть сообщения, которые обрабатываются базами данных.
Итоговая БД может поддерживать консистентный вывод, и одновременное поддерживать внутри себя консистентное состояние.


Домашнее задание:
Есть развёрнутый Clickhouse с ядром Merge 3, в котором будем проводить эксперимент:
У нас есть логистика на N складов. У нас в БД вместо того, чтобы работать в общем режиме, у нас как-то соединён склад (графом) - этот граф сами придумаем (N до 50-штук, небольшое). Сгенерируем количество запросов на перемещение объектов между складами. Есть N складов, есть k-типов объектов. Тип объектов тоже немного, k < 100. 
Есть время 1 перемещения между складом A и B. 
Есть стоимость одного перемещения. 
Есть стоимость потери - то, сколько оно будет стоить при утрате.

Есть 100000 объектов k-типов, распределённых по складам неравномерно (как угодно, случайным образом нагенерить).
Наша задача - с каждого склада есть команды (кто-то хочет забрать какой-то объект с какого-то склада). У нас на складах ничего нет как правило. Наша задача - построить оптимальную карту перемещений, при этом у нас есть лист сбора:

Запись: {№ записи, тип объекта к, T - количество объекта}

Таких записей в заявке
$k\cdot T = 100000$

За одно действие мы хотим получить (читая первую заявку) - хотим это получить на складе с номером N, и если их нет там, мы ждём, пока туда будет доставлено всё. 
Наша задача - за наименьшее количество перемещений обеспечить минимальную задержку при перемещении.
Первая задача: мы знаем график выдачи, наша задача - таким образом переместить объекты между складами, чтобы обеспечить наименьшую задержку. Наши заявки выполняются последовательно и у нас образуется новая заявка

5

Минимум 6 тестов случайных, на этих 6 тестах наше перемещение (программа распределения) должна давать стабильно наименьшие штрафы

#### Вывод

#### Ссылки
Source Material and other similar notes
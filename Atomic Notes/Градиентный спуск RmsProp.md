**Created at**: [13-11-2025 21:50]
**Tags**: #Градиентный_спуск #ML
![[RmsPropLogo.png]]
#### Рассматриваемое знание
В [[Atomic Notes/Градиентный спуск AdaGrad]] необходимо было добавить **экспоненциальное затухание** для улучшения работы градиентного спуска.
Рассмотрим такие формулы:
$$v = \gamma v + (1 - \gamma)g^2, \gamma \in (0, 1)$$
$$x = x - h\frac{g}{\sqrt v}$$
Если $\gamma$ = 1, то мы не учитываем $g^2$ никак, если $\gamma$ = 0, то мы не учитываем v никак.
#### Принцип работы
Пусть находимся на некоторой итерации $g_t^2 (1-\gamma)\gamma^{T-t}$, 
Чем меньше $\gamma^{T-t}$, тем меньше всё слагаемое. А $\gamma^{T-t}$ меньше тогда, когда $T-t$ меньше, и это происходит, чем больше итерация t. Это называется **экспоненциальным затуханием**, поэтому будем помнить относительно недавнюю историю градиентов, а не всю полностью.

Мы хотим, чтобы единицы измерения у градиентов были одинаковые, чтобы мы спускались по всем переменным примерно с одинаковой скоростью. Для того, чтобы этого достичь - мы делим их на значения, которые были в истории. Большие значения будут делиться на большие, маленькие на маленькие, поэтому произойдёт выравнивание - и это работает, мы спускаемся примерно с одинаковой скоростью по всем координатам.
При этом с инерцией не работаем в этом методе.
#### Вывод и с чем не сработает
Не работает с ямами. Для разрешения проблемы необходимо использовать и инерцию, и приведение единиц измерения. Решение в [[Градиентный спуск Adam]]
#### Ссылки
1. [ИИБД - Машинное обучение 1 курс - 1 лекция - 2025.10.20 - часть 1 градиентный спуск (не с начала)](https://www.youtube.com/watch?v=fAEjFVv3sOk&list=PLWc6uq3QodNy5HX7zTTBXsJYWAwbw4q3M&index=1)
2. [[Градиентный спуск в машинном обучении]]
3. [[Градиентный спуск Adam]]
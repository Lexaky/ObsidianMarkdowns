**Tags**: #Градиентный_спуск #ML
#### Рассматриваемое знание
Алгоритм пытается вычислить вторую производную, использует её, чтобы ускорить сходимость. Но вторую производную вычислять очень дорого и долго.
Вторая производная - это Гессиан - это матрица такого вида:

$$H =
        \begin{Bmatrix}
        \frac{\partial f}{\partial x_i \partial x_j}
        \end{Bmatrix}_{i,j=1}^n
$$
На диагонали будет производная $\frac{\partial f}{\partial x_i^2}$ , а на остальных местах сначала будет производная по одной переменной, а потом по другой. В нейросетях количество переменных измеряется миллиардами, поэтому такая матрица получается миллиард на миллиард. К сожалению, поместить такое в оперативную память не получается, поэтому мы не может позволить себе методы второго порядка, они недопустимы в машинном обучении и нейросетях. Также в линейных регрессиях всегда не получается из-за большого количества входных данных.

В Sofia на самом деле есть оценка, естественно Гессиан не весь считается. Авторы алгоритма приводят несколько примеров, один из примеров это Adam - если мы возьмём их оценку, применим их к задаче бинарной классификации, то получается в точности Adam. Но есть определённые хитрости, которые можно использовать, однако Sofia не победила даже с ними.
При обучении очень больших моделей чаще всего используют просто градиент с моментом, без деления на $g^2$, то-есть даже не Adam.

#### Ссылки
[[Матрица Гессе]]

**Tags**: #Градиентный_спуск #Момент_в_градентном_спуске #ML
#### Рассматриваемое знание
У нас снова есть $x_0$, у нас есть $v_0 = 0$, $m_0 = 0$
Итеративно:
$$m_t = \beta_1m_{t-1} + (1-\beta_1)g$$
$$v_t = \beta_2v_{t-1} + (1 - \beta_2)g^2$$
$$x_t = x_{t-1} - h \frac{m_t}{\sqrt{v_t + \epsilon}}$$
Но на самом деле авторами было замечено, что v и m сначала очень маленькие, потому что пока мы не проведём какое-то количество итераций, они будут равны нулю.
Поэтому в формулы добавляется следующее:
$$\hat m_t = \frac{m_t}{1 - \beta_1^t}$$
$$\hat v_t = \frac{v_t}{1 - \beta_2^t}$$
И потом в формуле $x_t$ используем изменённые значения:
$$x_t = x_{t-1} - h \frac{\hat m_t}{\sqrt{\hat v_t + \epsilon}}$$
#### Почему это работает?
Обычно $\beta_1$ и $\beta_2$ близки к 1. $\beta_1$ это обычно около 0.999. А $\beta_2$ это 0.9.
Что происходит? - мы делим момент в начале обучения и квадрат градиентов на число, близкое к нулю - так мы увеличиваем значение. Потом, когда обучение продолжается, t возрастает, мы начинаем из 1 отнимать 0. Теперь $m_t$ будет просто похоже на $m_t$, т.к. мы просто делим на единицу. Это помогает в начале обучения обучение ускорить, чтобы разогнаться. А в конце обучения это никак не влияет, потому что мы делим на 1.

Помимо этого существуют и другие алгоритмы:
[[Градиентный спуск Sofia]], ==AdeDelta, Demon== - они в том или ином виде улучшают эти две идеи, которые используются в Adam'е. То-есть они вносят определённые риски.

По умолчанию стоит использовать Adam, он сходится почти всегда и почти хорошо. 

Существует также ==AdamV== - в нём добавлена регуляризация, которая заключается в наличии штрафа за большие значения $x$:
$$L = L + \alpha ||w||_2$$
То-есть оказалось, что если веса большие, то модель переобучилась и её нужно штрафовать за большие веса, чтобы она пыталась их сделать как можно поменьше. AdamV выделяется отдельно, т.к. регуляризация не попадает в $g$, её считают отдельно и выводят, чтобы она не накапливалась в градиенте, в моменте и в квадрате, - а в остальном такой же Adam, как и остальные.

#### Вопрос
А как выбирать h (learning rate)? - есть наиболее стандартные варианты: $10^{-3}$. Эмпирически оказалось, что это работает часто. Но сейчас так уже не делают, а сейчас делают специальный learning rate, который изменяется во время обучения, то-есть у него тоже добавляется индекс t. 
#### Правило изменения learning rate
Чем дальше в обучении - тем меньше.
Сначала начинаем с большого шага, чтобы посмотреть на большее пространство параметров. То-есть у нас есть некоторое пространство, в котором находится минимум, надо попрыгать большими шагами и найдём где-то похожий на минимум. А когда мы поняли, где минимум - нужно в этой области ходить маленькими шагами, чтобы в эту область спуститься.
![[Adam_jumps.mp4]]

Но так тоже не всегда работает. Поэтому сейчас применяется **WormMap**? - когда мы начинаем с $g$ = 0, а потом увеличиваем его до какого-то значения, а после достижения этого значения спускаемся обратно. Почему так нужно сделать? - потому что данные в модель мы подаём не все сразу, т.к. их бывает слишком много. Если наш h будет слишком большой, то мы очень быстро спустимся в минимум до данных, которые мы видим в начале; а потом до данных, которые будут дальше, спускаться в минимум будет посложнее. Поэтому сначала не разрешим модели слишком далеко прыгать, сначала маленькими шагами посмотрит на все данные, а потом будет прыгать посильнее, когда начнёт иметь представление о том, что происходит вообще. Поэтому сейчас график *learning rate* выглядит так:
![[Adam_cos.png]]
WormMap называется линейным, а Loss Pad называется косинусным. Мы хотим за пол периода косинуса чтобы lr спустился до 0.

#### Ссылки
[[Градиентный спуск Sofia]]
[[Градиентный спуск с инерцией]]
[[Градиентный спуск AdaGrad]]


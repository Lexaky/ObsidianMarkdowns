**Tags**: #Градиентный_спуск #ML
#### Рассматриваемое знание
Смысл: 

#### Проблема на примере
Рассмотрим такой график `Loss function`
![[AdaGrad_3d_loss_function.mp4]]
Предположим, что мы выбрали случайную точку $x_{start}$ и начинаем движение с неё. 
В случае использования [[Градиентный спуск с инерцией]] возникают проблемы:
мысленно разделим красный маршрут на 2 части - до угла изгиба (1) и после угла изгиба (2), также введём оптимальный маршрут (0).
![[AdaGrad_3d_loss_function.png]]Мы будем идти по маршруту (1) с инерцией, а потом пойдём по маршруту (2). Проблема в том, что оптимальный маршрут (0) по диагонали, но по ней не получается, потому что по оси (1) градиент больше. Это **проблема единиц измерения**.
У нас производная по каждой переменной в разных единицах измерения (допустим, одна в метрах, другая в километрах, просто у них разный масштаб).
Градиент, решающий проблему - **AdaGrad**.
Теперь мы будем считать $v = v + \gamma g^2$ (*это не инерция*, идея другая, просто переменная называется так же)
Тогда $x = x - h \frac{g}{\sqrt v}$
Суть заключается в следующем: для тех переменных, которые имеют большой градиент, число v будет большим, тогда мы большой градиент поделим на больше число. А малый градиент на маленькое число.
Таким образом мы приводим градиенты к одной единице измерения.
#### Плюсы
Алгоритм сходится быстрее, чем градиентный спуск.
#### Проблема алгоритма
Алгоритм всегда сходится - число v всегда возрастает (т.к. мы всегда прибавляем какое-то число g в квадрате). То-есть наш алгоритм имеет ограниченное количество шагов, и в какой-то момент из x мы будем отнимать 0 - значит наш алгоритм сошёлся, но мы получим не обязательно минимум и, может быть, даже не значение рядом с минимум.
Мы накапливаем v постоянно, оно растёт и ничем не ограничено, и в итоге мы из x будем отнимать очень маленькие числа, почти 0.
#### Что делать?
Добавить экспоненциальное затухание. [[Atomic Notes/Градиентный спуск RmsProp]]

#### Ссылки
[[Градиентный спуск RmsProp]]
[[Градиентный спуск с инерцией]]
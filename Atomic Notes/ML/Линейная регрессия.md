**Tags**: #MSE #Функция_ошибки #Гомоскедастичность #Квартет_Эскомба #Полиномиальная_регрессия #Линейная_регрессия  #ML
#### Рассматриваемое знание
$$ y = w_1x_1 + w_2x_2 + ... + w_nx_n + b = \sum_{i=1}^nw_ix_i + b = Wx + b = \hat W [x, 1] = w_1x_1 + ... + w_nx_n + w_{n+1}$$
$$W \in \Bbb R^n, \hat W \in \Bbb R^{n+1}$$
Сумма произведений весов на координаты, потом снова сумма произведений весов на координаты + bias, потом скалярное произведение весов на $X + b$. Представим всю нашу задачу в виде одного скалярного произведения, единичка обозначает, что $w_{n+1}$ будет умножено на 1. То-есть мы расширили наш $x$ и избавились от $b$.

Входные данные - это наш X - это множество array, а Y - это R
$$\bar y = \{ \sum_{w_{i, j}}^nx_i + b_j \}_{j=1}^m$ = W_x \in \Bbb R^n$$
Теперь нарисуем график, есть какие-то точки.
![[Линейная_регрессия_график_точек_и_прямых.png]]
Если построить график модели, то получится, что она возрастающая и при этом у неё среднее расстояние до точек минимально. Тогда `Loss function`:
$$L = MSE = \frac{1}{\#\{X\}}\sum_{i=1}^{\#\{X\}}(y - \hat y)^2$$
$$RMSE = \sqrt {MSE}$$
$$MAE = \frac1n \sum{|y = \hat y|}$$
Мы не используем MAE, потому что тяжело считать производную, а с квадратом всё хорошо.
#### Какие требования к данным нужны, чтобы модель хорошо работала?
- Ошибка примерно постоянна везде (дисперсия одинакова) - **гомоскедастичность**.
**Гомоскедастичность** - свойство, означающее постоянство условной дисперсии вектора или последовательности случайных величин. Однородная вариативность значений наблюдений, выражающаяся в стабильности, однородности дисперсии случайной ошибки регрессионной модели — дисперсии одинаковы во все моменты измерения. Противоположное явление носит название гетероскедастичности. Является обязательным условием применения метода наименьших квадратов.
![[Homoscedasticity.png]]
![[Heteroscedasticity.png]]
![[Homoscedasticity_own_example.png]]
На последнем изображении график функции гетероскедастичный - потому что в близких точках ошибка маленькая, а в удалённых точках ошибка большая, а мы хотим, чтобы ошибка была примерно одинаковая везде. Чем более данные соответствуют этому условию, тем лучше будет работать линейная регрессия.

Ниже 4 картинки, они называются **Квартет Эскомба**.
![[Anscombe.svg.png]]
Эти 4 картинки рисуют, потому что у всех этих наборов точек одинаковые дисперсии, но при этом они явно разные. Это говорит о том, что смотреть только на среднюю дисперсию плохо. Два одинаковых набора данных с одинаковой средней дисперсией могут не подлежать одному и тому же распределению.

#### Как посчитать веса?
У нас есть линейная регрессия: 
$$y = \theta x$$при этом может быть без $b$, потому что мы добавили единичку в x.
Пусть $L = MSE$, как теперь найти веса?
1) Случайный перебор
2) Баесовская оптимизация
Это всё  - не хорошие идеи, т.к. долго, дорого, хуже гарантия сходимости.
Попробуем взять градиентный спуск - это нормальный вариант.
Другой способ:
Попробуем посчитать производную. Мы говорили, что если это минимум, то производная L по $\theta$ равна 0:
$$min \rightarrow \frac{\partial L}{\partial \theta} = 0$$
Записываем нашу ошибку:
$$L = MSE = \frac{1}{\#\{X\}}\sum_{i=1}^{\#\{X\}}(y - \theta x)^2$$
Перепишем попроще, чтобы было проще думать:
$$(y - \theta x)^2$$
Теперь у нас `y` *- вектор*, а `x` *- матрица*.
Раскроем квадрат:
$$(y - \theta x)^2 = y^Ty - y^T\theta x + \theta ^ Tx^Tx\theta - x^T\theta^Ty$$
Можно сократить выражение, так как $x^T\theta^Ty = y^T\theta x$
Тогда получится выражение:
$$y^Ty - 2y^T\theta x + \theta ^ Tx^Tx\theta$$
Возьмём производную по $\theta$ (как будто это число, дифференцируем вслепую):
$$(y^Ty - 2y^T\theta x + \theta ^ Tx^Tx\theta)_\theta^{'} = -2y^Tx + 2\theta^Tx^Tx$$
Делим на 2:
$$2\theta^Tx^Tx - y^Tx = 0$$
$$\theta^Tx^Tx - y^Tx = 0$$
Перенесём $y^Tx$ за знак тождества. Транспонируем выражение:
$$x^Tx\theta = x^Ty$$
Выразим $\theta$, cначала нужно умножить слева на $(x^Tx)^{-1}$:
$$\theta = (x^Tx)^{-1}x^Ty$$
Оказывается, что минимум функции MSE для задачи линейной регрессии это такое $\theta$, которое мы получили.
**Утверждение.** Методы оптимизации для линейной регрессии этим не пользуются.
Этим не пользуются, потому что вычисление обратной матрицы это достаточно тяжёлая операция - итеративная и долгая. Эта матрица может быть плохо определена, может быть вообще не быть обратной. Например, когда есть линейно зависимые параметры. $x_1$ линейно зависимое от $x_2$, когда $x_1$ = $\alpha x_2$.

Оказывается, что посчитать линейную регрессию можно аналитически, но это требует больше вычислений и работает не всегда.
Поэтому пользуются специальными алгоритмами типа градиентного спуска или типа MSE, которые про это знают и не так подвержены этому.

`Какой будет эффект, если у нас есть 2 колонки линейно зависимые, мы посчитали для неё лин регрессию градиентным спуском, а потом одну из колонок выбросили и снова посчитали?`
- В случае если оба, то линейная регрессия раздаст им один и тот же вес пополам каким-то образом. А если один выбросим, то один и тот же вес всё-равно добавит в нашу модель, поэтому будет один общий коэффициент для этого. 
Это легко показать:
Пусть $x_1 = \alpha x_2$
$$y = w_1x_1 + w_2x_2 = w_1\alpha x_2 + w_2 + x_2 = x_2(w_2 + \alpha w_1)$$
#### Дополнительный материал
Есть линейная регрессия $y = ax + b$
Нужно доказать, что $y = ax^2 + bx + c$ тоже линейная регрессия.
Пусть $x^2 = x_1, x = x_2$, тогда:
$$y = ax^2 + bx + c = ax_1 + bx_2 + c$$Получилась линейная регрессия. Вывод: **Полиномиальная регрессия тоже линейна**.

Для улучшения можно рассмотреть [[Линейная регрессия Lasso]]

#### Ссылки:
[[Градиентный спуск Adam]]
[[Линейная регрессия Lasso]]
[[Метод наименьших квадратов (MSE)]]
[[Градиентный спуск в машинном обучении]]

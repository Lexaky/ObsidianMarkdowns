**Tags**: #Градиентный_спуск #Функция_ошибки #ML
![[Градиентный_спуск_в_машинном_обучении_лого.png]]
#### Рассматриваемое знание
**Смысл**: Главная задача машинного обучения — минимизировать **функцию потерь (Loss Function)**. Эта функция измеряет, насколько "ошибаются" прогнозы нашей модели. Стремление $Loss \rightarrow min$ означает, что мы хотим сделать модель как можно более точной.

Чтобы функция достигала минимума в некоторой точке, её **производная (градиент) в этой точке должна быть равна нулю**.
$$x \rightarrow min \Rightarrow \frac{\partial L}{\partial \theta} = 0$$
- `θ` — здесь обозначает **параметры модели** (веса и смещения), которые мы настраиваем.
- $\frac{\partial L}{\partial \theta}$ — это **градиент**, вектор частных производных функции потерь по каждому параметру. Он показывает направление наискорейшего _роста_ функции.

#### Суть алгоритма в математическом виде
$$\exists \alpha \gt 0: f(x) > f(x - \alpha \frac{\partial L}{\partial x}), \frac{\partial L}{\partial x} \neq 0$$
Смысл: Эта запись формально описывает суть алгоритма. Она гласит: "Существует такой размер шага `α > 0`, что если мы сделаем шаг в направлении, **противоположном градиенту**, значение функции **уменьшится**.
- `α` — **learning rate** (скорость обучения).
- -$\frac{\partial L}{\partial \theta}$ — движение _против_ градиента, то есть в направлении наискорейшего _спуска_.

Это условие гарантирует, что при достаточно малом шаге мы будем двигаться к минимуму.

#### Результат оптимизации
**Смысл:** Когда градиент станет равен нулю (условие минимума выполнено), мы найдём оптимальные параметры модели `θ`. `S(x, θ)` — это, условно, **решающее правило** или **функция**, которую определяют найденные параметры `θ` на основе данных `x`.
$$\frac{\partial L}{\partial \theta} = 0 \Rightarrow \theta = S(x, \theta)$$
Чтобы убедиться, что в найденной точке (где градиент равен нулю) находится именно **минимум**, а не максимум, нужно проанализировать вторую производную (или **матрицу Гессе** для многомерного случая).
- Если вторая производная **положительна** (или матрица Гессе **положительно определена**), то это локальный минимум. $$\frac{\partial^2L}{\partial\theta^2} \geq 0$$
#### Алгоритм градиентного спуска
Алгоритм начинает работу со случайно выбранных начальных значений параметров $x = x_0$.
Итерационный процесс (формула обновления параметров модели на каждом шаге t): $x_t = x_{t-1} - h \frac{\partial L}{\partial \theta}$ 
- h — это **learning rate** (скорость обучения).
- $\frac{\partial L}{\partial \theta}$ — градиент, вычисленный в точке $x_{t-1}$.

#### Условия остановки
1) Количество итераций
2) Значение метрики $\geq$ m
3) $||x_t - x_{t-1}|| \leq \epsilon$
4) Значение L $\leq \epsilon$ 
5) $||L(x_t) - L(x_{t-1})|| \leq \epsilon$
6) $||L(x_m) - L(x_{t})|| \leq \epsilon$

**Условие остановки 5** является определённым исключением, потому что функция L может выглядеть так:
![[Градиентный_спуск_убывающая_экспонента.png]]
Абсолютное значение разницы между значением функции потерь на текущем шаге $L(x_t)$ и на предыдущем шаге $L(x_{t-1})$ меньше или равно некоторому малому числу ε.
Мы останавливаем алгоритм, когда функция потерь **практически перестала уменьшаться** от итерации к итерации. Мы считаем, что если за один шаг мы улучшили модель всего на какую-то ничтожно малую величину, то мы, скорее всего, достигли "дна" (минимума) и дальнейшая оптимизация бессмысленна. Пример показывает, что минимум может быть ниже постоянно, при этом шаг уменьшается.

Каждое условие остановки желательно использовать в комбинации для достижения лучшего результата.

**Условие 1** лучше использовать всегда в дополнение к остальным, потому что метод градиентного спуска не гарантирует, что мы достигнем **условий 2-6**.

**Условие 6** говорит о том, что разница функций в точках $x_m$ и $x_t$ минимальная, то-есть функция не изменяется. $x_m$ - это предыдущая точка минимума.

#### Факты
- Если `Loss Function` резко возрастает, то это говорит о том, что в программе где-то баг. Это возможно в случае, например, если `learning rate` большой, или если входные данные были перепутаны с выходными.

#### Вывод
Градиентный спуск на самом деле в таком виде не работает (так модели не обучают), потому что могут существовать локальные минимумы и максимумы. 
![[Градиентный_спуск_известная_проблема.png]]
Если искать точку минимума, начиная с левого края для данной функции потерь, то мы достигнем ближайшего минимума и алгоритм завершит работу (при этом, только в случае, если шаг большой, то второй минимум достижим). Но на самом деле далее есть точка минимума, которая располагается ниже найденного в правой части графика - это и есть изъян рассмотренного алгоритма.

Помимо этого существуют подобные функции, и если выбирать случайно начальные точки, то найти пропасть (настоящий минимум) будет тяжело. 
![[Градиентный_спуск_пропасть.png]]

Для усовершенствования алгоритма предлагается накапливать инерцию точки x:
[[Градиентный спуск с инерцией]]
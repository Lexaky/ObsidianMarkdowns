**Created at** [29-11-2025 13:30]
**Tags**: #ML 
# KNN

#### Рассматриваемое знание
KNN - метод k-ближайших соседей, применяемый в задачах регрессии, классификации. 
Есть какое-то кол-во точек, раскрашенных в разные цвета (классы). Потом возникает новая точка, для которой необходимо определить, к какому цвету (классу) она относится. Сделаем предположение о том, что точки одинакового класса находятся рядом друг с другом.
Ищем k ближайших точек (объектов) (допустим 4 шт. из примера 1). Считаем, сколько точек из каждого класса; считаем, сколько классов больше, делаем вывод о том, что точка принадлежит данному классу.
1
Проблемы метода: 
Посчитать расстояние до всех точек *вычислительно долго*.
Нужно помнить о проблеме размерности - когда размер пространства становится большим, то все расстояния становятся почти одинаковыми.
Стандартное Евклидово расстояние в больших размерностях работает плохо. 
Увеличение размерности, все точки выбранные в гиперкубе, находятся примерно на одинаковом расстоянии от его центра, хотя точки все разные.

Необходимо прибегать к $L_p$ метрике:
$$||x||_p = \sqrt[p]{\sum{|x_i|^p}}$$
При увеличении размерности расстояние между точками становится почти неразличимым - можно уменьшать размерность (но тогда мы потеряем информацию, это плохо). Можно помимо $L_p$ метрики использовать другую метрику:
Косинусное расстояние. Для векторов, которые имеют длину 1:
$$1 - xy$$
исходит из:
$$cos (\alpha) = \frac{xy}{||x||\ ||y||}$$
Косинусное расстояние ограничено от 0 до 2, т.к. если векторы разнонаправлены, то 1 - (-1) = 2
Пусть вектор имеет норму 1. Если они одинаковые (1-cos(alpha)), то cos угла между ними будет равен ...?
Проблема с метрикой решена. что делать с кол-вом точек?
Нужно уменьшить их количество, какие точки можно выбросить? - в множестве некоторые точки точно будут лишними. 
В центре кластеров, т.к. если точки попадают в центр кластера, то у неё вокруг уже куча соседей есть, которые определят её класс. А точки, которые попадают на границу между кластерами - будет неизвестно, к какому классу она будет определена.
5
Как определить, насколько плотно точка находится в кластере? 
С помощью алгоритма Харта:
1. Заводим пустое множество оставшихся точек $U=\emptyset$
2. Перебираем все точки. Для каждой точки $p$:
	Если ближайшая точка из $U$ - тот же класс, что и $p$, то $p$ выбросить.
	Иначе $p$ добавить в $U$.

Т.е. если мы пришли в точку, и она в том же классе, который мы выбрали, и классифицируется правильно, то эта новая точка уже не нужна. А если неправильно, то нужно добавить в множество U, чтобы классифицировалась правильно.
В каком порядке нужно обходить все точки? Начинать нужно с граничных классов. Необходимо их обойти в порядке "уменьшения граничности".
Как определить, что точка у границы?
$\alpha$ - коэффициент "пограничности". Считается:
$$\alpha(x) = \frac{||\hat x - y||}{x - y}$$
$y$ - ближайшая к $x$, но другого класса
$\hat x$ - ближайшая к $y$, то того же класса, что $x$.
$\alpha(x)\in[0; 1]$
Хотим определять, насколько близко к границе находится точка. Как узнать, что точка близко к границе? - находим расстояние от точки класса 1 до ближайшей точки класса 2, потом от точки класса 2 расстояние до другой точки класса 1, вычисляем расстояния между ними, оцениваем его и понимаем, какие точки ближе к границе.

Что, если точками не хочется жертвовать? Можно применить аппроксимированный KNN (сторонняя доп. тема). Можно построить дерево - воспользуемся структурой данных, которая позволяет наложить дерево, которое позволит также и оценивать расстояние до точек.
Структуры данных позволяют находить точки быстрее, чем полный перебор.

#### K-D tree. Ball tree
Оба строятся за $O(nlog(n))$ операций, запросы в худшем случае займут $O(n)$, а в лучшем $O(log(n))$.
K-D дерево разделяет пространство на плоскости по одной из координат. А в Ball tree каждый узел задаётся радиусом, и все точки в этом радиусе ...
Все точки, расстояние которых больше радиуса от ноды, ...

Как строится K-D дерево? Ball tree строится так же.
Пусть есть точки
7
В каком-то порядке расставляем координаты для точек, выбираем координату, считаем по ней медиану. 
8 делим плоскость на 2 части
Запомнили координату, по которой сделали разделение.
9 повторить для каждой оставшейся части (по следующей координате, которую мы составили)
Для каждой части продолжаем деление (если координаты закончились, то заново по x, y и так далее). 
Разделили плоскость на кубы, которые каким-то образом разделили точки.
Есть первая вершина, с помощью постоянных вопросов - с какой стороны находится искомая точка? - придём к её координатам. Если расстояние от искомой точки до границы меньше, чем от искомой до ближайшей к ней, то возможно с другой стороны границы есть точка, которая может быть ближе к искомой точке.
10

Каким образом искать k-ближайших, а не 1? - можно хранить набор точек с расстояниями. Если наименьшее расстояние от выбранной точки до медианы меньше ...

Проблемы KD-дерева: 
Размерность - `пояснение в записи`.
Ball tree строится примерно так же, но `пояснение в записи`.

#### Взвешенный KNN
11
в данном случае точка будет классифицирована к правому классу по методу $3NN$. Предлагается взвешивать точки по расстоянию. Для ближайшей левой точки расстояние минимальное, считаем его как $\frac{1}{p_1}$. До правых как $\frac{1}{p_2}$ и $\frac{1}{p_3}$. Можно посчитать общее расстояние $p$ и использовать его в числителе вместо 1 ($\frac{p}{p_1}$, $\frac{p}{p_2}$, $\frac{p}{p_3}$).
Теперь когда считаем количество, считаем сумму коэффициентов для каждого класса и выбираем тот класс, у которого сумма коэффициентов наибольшая.
KNN можно делать с регрессией - выбирается среднее взвешенное или взвешенное на расстояние до точек.
#### Вывод

#### Ссылки
1. [[Кластеризация]]
2. 
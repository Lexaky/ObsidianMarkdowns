[2025-11-13 23:06]
**Tags**: #Без_градиента #ML
# Неградиентные методы оптимизации

#### Рассматриваемое знание
**Неградиентные методы оптимизации** (НМО) применяются для оптимизации функций, где:
- Градиент неизвестен, сложно вычислить или не существует.
- Функция может быть "шумной" или стохастической.
- Параметры могут быть как зависимыми, так и независимыми от входа

#### Основные методы
1. Переборный метод: Прямой расчёт функции в очень большом количестве точек для нахождения *min/max*. Неэффективен для высоких размерностей.
2. Тернарный поиск: Делать триплет графика, выбирать ту часть из 3, которая нам выгодна, повторять итерацию. Возможен, только если $f$ монотонно убывает и возрастает.
3. Байесовская оптимизация: "Умный" перебор. Строит вероятностную модель функции, предсказывает перспективные точки для вычисления и уточняет модель.
4. Метод отжига (Имитация отжига): Алгоритм, вдохновлённый термодинамикой. На ранних итерациях допускает переходы к худшим решениям (чтобы избежать локальных оптимумов), с течением времени вероятность таких переходов уменьшается.
5. Стохастические методы (упомянут "Синтакс метод"): Общее название для методов, использующих случайность в процессе поиска.

#### Цель
В многокритериальных задачах часто ищут **Парето-оптимальные решения** (решения, где нельзя улучшить один параметр, не ухудшив другой).

#### Вывод
**Неградиентные методы** — это инструмент для поиска экстремумов в сложных условиях, когда классические градиентные подходы неприменимы или неэффективны.

#### Ссылки
Source Material and other similar notes
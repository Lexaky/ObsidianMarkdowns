**Created at** [13-12-2025 13:32]
**Tags**: #ML 
# SVM

#### Рассматриваемое знание
Постановка задачи:
Есть определённая плоскость:
1
Есть точки, разделённые на 2 класса.
Задача: провести между ними прямую так, чтобы она хорошо их разделяла.
В классической постановке классы -1 и 1, а не 0 и 1. Решаем классификацию.

SVM строит классификатор по уравнению
xw - b = 0
Чтобы это построить, предполагаем, что у нас есть ещё две линии, касательные к классам:
1) xw - b = -1
2) xw - b = 1
классы -1 и 1, поэтому в уравнениях они в тождестве
Расстояние A мы хотим, чтобы было максимальное.
А = $\frac{2}{||w||}$
||w|| - норма вектора, длина вектора, расстояние.
Находим 2 плоскости так, чтобы расстояние между ними было наиболее большим.
Нужно сделать расстояние А как можно больше, а также чтобы за каждой плоскостью находились точки принадлежащего класса.

Получаем задачу:
хотим $\frac{2}{||w||} \rightarrow max$ (по сути это и есть функция лосс), при условии что для всех y: $y *(xw - b) >= 1$, это условие получается:
если y - класс точки (-1 и 1). Если класс -1, то точка должна находится наверху на графике 1. Чему равно уравнение, проходящее через эту точку? xw - b = -d, где d >= 1 (какое-то число) - оно находится где-то выше плоскости xw-b=-1. Пусть y=1, точки находятся внизу, проходим плоскость через точку, получается, что xw-b=d, x*w-b > 1, тогда y(xw-b) >= 1. Все точки находятся по сторону проходящих плоскостей.
*

Это называется hand-margin - это работает только в том случае, когда датасет (данные) линейно разделены.

В нашем случае возникает такие ситуации, когда в данных -1 или 1 возникают объекты из противоположного класса. В данном случае не выполняется условие. Эта формула y(xw-b) становится меньше 1.
Добавим в задачу ошибку (кси):
$y_i (x_i w -b) >= 1 - \psi$, и ошибку нужно минимизировать
$-C\sum_{i=1}^n \psi_i + \frac{2}{||w||} \rightarrow max$
2
Ошибку называют soft-margin
Обычно записывают так:
Хотим минимизировать 1/2 ||w||^2 + C... 
3
Оно таким образом записано наоборот.

#### Оптимизация задачи
Хотим $\frac 1 2 ||w||^2 + C \sum_{i=1}^n \psi_i \rightarrow min$,
$y_i (x_i w - b) >= 1 - \psi_i$
$\psi_i >= 0$
Если задачу решили, то как делать классификацию? Хотим классифицировать на -1 и 1, нужно взять знак:
$sign(xw\ -\ b)$

Градиентный спуск не подходит для оптимизации, потому что условий много. ГС-ку тяжело сообщить об условиях.

#### Метод множителей Лагранжа
Он нужен, чтобы получить решаемую задачу (двойственную от исходной, которая решаемая и поддаётся решению). ММЛ - метод получения двойственной задачи.
Чтобы получить задачу, нужно выполнить операции:
Нужно построить функцию Лагранжа, она зависит от всех переменных, которые были
4
берём функцию от w, $\psi$, и к ней прибавляем ограничения , ограничения представляем в виде
5
фи от чего-то = 0
Строим ф. Лагранжа, которая представляет собой исходную функцию + лямбда умноженную на все условия, которые у нас есть. Лямбды называются **множителями Лагранжа**.

Будем брать производную от функции Лагранжа по всем переменным.
6
Они все должны выполняться. Нужно решить систему.
из уравнения получается 
6.1
...
Оптимизируем мы множители Лагранжа.
Подставляем вывод 6.1 в формулу 3 вместо w:
7
Сводим к минимуму.
Также есть условие
8
по-другому оно не разрешается, поэтому это условие.
Вывели b
9

В итоге получается формула 7
Там написано лямбда 1, лямбда 2, yi, yj. Наши места модели будут сопоставляться с yi и лямбда, то-есть лямбды имеют коэффициенты каждой известной нам точки.
(здесь начал запись 2)
7 - это решение двойственной задачи. У каждого x свой вклад в коэффициенты плоскости. Некоторые из них будут равны 0.
Есть какие-то коэффициенты, есть какие-то y-ки.
Лямбды - это кэффициент.
Игрики - это 
если классы одинаковые, то будут положительные,
если классы разные, то будут отрицательные.
Иксы - это число, косинус угла между двух векторов - он тем больше, чем больше похожи вектора.

Можем менять только лямбды.
Если классы разные, то для наиболее разных векторов при одинаковых иксах лямбды должны быть меньше.
Хотим те точки делать важными, которые
Иксы, похожие друг с другом, имеют разные классы - они должны быть больше.
Если y-ки одинаковые (получаем 1), то хотим их максимизировать.
(Объяснение в записи)

10

Уравнение
сумма лямбд * y = 0 - это взяло из условия по b. Если производная по b = 0, то этого условия бы не было. Если мы сделаем как обычно - скажем, что x = [x, 1], а w = [w, b]. Получается, что в формуле нет свободного слагаемого, условие Лагранжа по нему = 0 (условие 6.2), и теперь можно делать градиент, только теперь на каждом шаге нужно лямбду ограничивать (если оно стало слишком маленьким, то его нужно занулить).

Теперь есть удобная функция, которая легко оптимизируема, которая везде сходится.
Это всё работает, когда точки примерно одинаково распределены.
Если
11
Точки линией не разделимы. Нужно:
Повысить размерность. Предположим, что существует пространство, в котором эти точки линейно разделимы. Найдём функцию, которая отображает точки 12
Эта функция точки раздвигает. В том пространстве разделим линейно. Можно решить задачу в том пространстве, тогда вместо $x_i * x_j$ пишем $\phi(x_i) * \phi(x_j)$ в 7 формуле. Сначала идём в то пространство, узнаём на какой прямой они находятся, и говорим ответ. $\phi(x_i)*\phi(x_j) = k(x_i, x_j)$ -это называют ядром. Функция, которая вычисляет как векторы похожи друг на друга, бывает разной.

Самое простое:
$k(x_i, x_j) = x_i * x_j$
Можно придумать полиномиальное ядро:
$k(x_i, x_j) = (x_i * x_j, d)^k$, - чем эта величина больше, тем больше всё. Для степени k=2 - 10-15 координат в разложении на $\phi$.
Ещё можно RBF (радиальная базисная функция) ядра:
$k(x_i, x_j) = e^{-\gamma||x_i-x_j||^2}$, - на каком радиусе от x мы находимся. Если рассматривать как Гауссово распределение, то берём среднее, смотрит на вероятность, попали ли мы в распределение.
У этой функции есть проблемы - она не представима в виде скалярного произведения $\phi$. Мы говорим о пространстве, где разделимы, тогда произведение двух функций будет представлять ядро в виде нашей модели. У этого ядра не существует разложения иксов в конечномерном пространстве. В бесконечномерном получится. За этим следуют ограничения: ...

13

Ещё есть *tan* hyperbolic:
$k(x_i, x_j) = tanh(ax_ix_j + b)$

Добавление пространства и решение в нём называется **kernel trick**

#### Вывод

#### Ссылки
Source Material and other similar notes
**Created at** [17-12-2025 15:39]
**Tags**: #Математическая_статистика 
# Элементы теории вероятности и математической статистики

#### Статистические методы обработки данных
1. Параметрические
	Более известны (из классического курса статистики), мощные
2. Непараметрические
	Более грубые, умеют работать с "неидеальными данными"

#### Типы данных - параметрические и непараметрические
Параметрические данные:
1. Шкала интервальная, непрерываня
2. Объём выборки большой (30 и более объектов)
3. Распределение близко к нормальному

Параметризация данных:
1. Подобрать непрерывную шкалу измерений
	Листья: большие\маленькие, размер в см.
	Цвет: название \ RGB
	Пол - ?
2. Объём выборки увеличить
3. Преобразование распределения, улучшающее его форму

#### Улучшение распределения
1. Логарифмирование
	$log(y+1)$ - колокообразное с длинными хвостами
2. Квадратный корень
	$\sqrt(y)$
3. Обратное
	$\frac1{y+1}$ - стабилизирует дисперсию
4. Логит
	$log(\frac p{1-p})$
![[ЭТВМС(продолжение)_улучшение_распределения.png]]

#### Непараметрические характеристики распределения
Робастные аналоги:
Для **математического ожидания** (*среднего*) - это **медиана**
Для **стандартного отклонения** - это **квартили**

Примеры:
Зарплаты сотрудников небольшой фирмы:
```
salary <- c(21, 19, 27, 11, 102, 25, 21)$
mean(salary) = 32.3; sd(salary) = 31.16
median(salary) = 21; IQR(salary) = 6
```
Медиана - середина упорядоченной выборки (для чётных $n: 1/2$ суммы двух средних членов вариационного ряда).
Квартили - значения, которые отсекают $0\%, 25\%, 50\%, 75\%, 100\%$ всего распределения.
Межквартальный разброс - расстояние между 1-м и 2-м квартилем.
![[ЭТВМС(продолжение)_зарплаты_график.png]]

#### Представление характеристик распределения в виде Диаграммы размаха или "ящика с усами" (диаграмма Тьюки). Выбросы

Границами ящика служат первый и третий квартили (25-й и 75-й процентили соответственно).
Линия в середине ящика - медиана (50-й процентиль).
Концы усов - края статистически значимой выборки (без выбросов), могут определяться несколькими способами.
Наиболее распространённые значения, определяющие длину усов:
Минимальное и максимальное наблюдаемые значения данных по выборке (в этом случае выбросы отсутствуют);
Разность первого квартиля и полутора межквартальных расстояний;
Сумма третьего квартиля и полутора межквартальных расстояний (в этом случае присутствуют выбросы).
![[ЭТВМС(продолжение)_ящик_с_усами_Диаграмма_Тьюки.png]]

#### Тестирование гипотез
Данные наблюдений случайны $\rightarrow$ выводы в некоторой степени случайны, даже по репрезентативной выборке нельзя сделать вывод на $100\%$ точный.
Выдвигаем гипотезы, вычисляем их вероятность.
Мы ничего не можем доказать, только может лишь что-то опровергнуть.
Статистический тест выдвигает, как правило, 2 гипотезы:
$H1$ - то, что хочется доказать;
$H0$ - альтернатива (противоположность $H1$) - что нужно опровергнуть.
Как правило, является предположением от отсутствии чего-либо;
Отвергая $H0$, можем принять $H1$.
Если отвергнуть $H0$ нельзя, то придётся её принять.

#### Статистические ошибки I-го и II-го рода
Ошибка первого рода состоит в том, что ***верная*** гипотеза $H0$ будет отвергнута - ложноположительный исход, нашли несуществующую закономерность.
Вероятность допустить ошибку I-го рода называют уровнем значимости $\alpha$ (*p.value* в языке R)

Ошибка второго рода состоит в том, что неверная гипотеза $H0$ будет принята - не заметить существующую закономерность - ложноотрицательный исход.
Вероятность допустить ошибку II-го рода обозначают $\beta$.
Число $1-\beta$ равное вероятности отверждения неверной нулевой гипотезы $H0$ называется **статистической мощностью критерия**.


| Генеральная совокупность<br>Выборка | Верна $H0$    | Верна $H1$     |
| ----------------------------------- | ------------- | -------------- |
| Принимаем $H0$                      | +             | Ошибка II рода |
| Принимаем $H1$                      | Ошибка I рода | +              |

#### Статистические ошибки I-го и II-го рода
![[ЭТВМС(продолжение)_уровни_значимости.png]]
Как правило, задают уровень значимости: $\alpha = 0.05$, реже $\alpha=0.01$ и $\alpha = 0.1$

При уменьшении вероятности $\alpha$ (отвергнуть правильную гипотезу) - растёт вероятность $\beta$ - принять неверную гипотезу (при прочих равных условиях).

При увеличении объёма выборки n становится выше мощность теста:
	При достаточно больших выборках даже небольшие различия окажутся статистически значимыми;
	При малых выборках даже большие различия выявить весьма трудно.
Чтобы избежать эффектов "слишком малой" и "слишком большой" выборок, априорное задание обеих характеристик точности критерия (уровня значимости $\alpha$ и ошибки второго рода $\beta$) необходимо увязывать с объёмом имеющихся данных.

#### Проверка на нормальность
Предположение о нормальности - одно из основных для многих статистических процедур:
1. t-tests,
2. анализ линейной регрессионной модели
3. дискриминантный анализ
4. дисперсионный анализ (ANOVA)
5. И др.
Если предположение о нормальности нарушено, интерпретация и выводы могут быть ненадёжными или неправильными.

Распространённые методы проверки гипотезы о том, что случайная выборка независимых наблюдений размера n происходит из популяции с нормальным распределением.
**Графические методы**:
1. Гистограммы
2. Диаграммы рассеяния (ящик с усами)
3. QQ-графики
**Численные методы**: расчёт индексов асимметрии и эксцесса.
Формальные тесты на нормальность.

##### Графические методы. Гистограмма
![[ЭТВМС(продолжение)_гистограммы1.png]]
![[ЭТВМС(продолжение)_гистограммы2.png]]
![[ЭТВМС(продолжение)_гистограммы3.png]]

##### Графические методы. QQ график
График QQ - это "квантиль-квантиль" график. На нём показана связь между наблюдаемыми значениями переменных и их теоретическими квантилями.
Если наблюдаемые значения попадают на прямую линию, то теоретические распределение хорошо подходит к наблюдаемым данным.

![[ЭТВМС(продолжение)_qq_plot.png]]

Квантиль - это число такое, что заданная случайная величина не превышает его с фиксированной вероятностью.
Например, $0.25$-квантиль - это такое число, ниже которого лежит примерно $25\%$ выборки.
Синоним квантиля - процентиль (percentile).

##### Построение QQ графика
1. Упорядочить данные и вычислить значения кумулятивного распределения (эмпирическую функцию распределения, $F(x)$);
2. График эмпирической функции распределения $=F(x)$ (верхний левый угол рисунка);
3. Аналогично - график ($F(x)$) стандартного нормального распределения $N(0, 1)$ (верхний правый угол рисунка)
4. Значения данных, соответствующие указанным квантилям, соединяются попарно и наносятся на график КК (нижний график на рисунке)
![[ЭТВМС(продолжение)_qq_plot_построение.png]]

#### Формальные тесты на нормальность
В статистической литературе описано более 40 тестов на нормальность.
Попытка разработки методов обнаружения отклонений от нормальности была инициирована Пирсоном (1895), который работал над коэффициентами асимметрии и эксцесса.
В статье (Razali, Wah, 2011) проведено сравнение эффективности четырёх формальных критериев нормальности, которые считаются одним из наиболее эффективных методов:
* Критерий Шапиро-Уилка (SW);
* Критерий Колмагорова-Смирнова (KS);
* Критерий Лиллифорса (LF);
* Критерий Андерсона-Дарлинга (AD).

#### Сравнение эффективности 4-х формальных тестов
Сравнение мощности этих тестов проведено на основе моделирования Монте-Карло выборочных данных, полученных из альтернативных распределений: симметричному и асимметричному.
$10.000$ выборок различного размера - из каждого альтернативного симметричных и асимметричных распределений.
Мощность каждого теста определялась путём сравнения тестовой статистики с соответствующими критическими значениями.
Результаты показывают, что:
I. Тест Шапиро-Уилка является самым мощным тестом на нормальность;
II. За ним следует тест Андерсона-Дарлинга;
III. Тест Лиллифорса;
IV. Тест Колмагорова-Смирнова.
Мощность всех четырёх тестов низкая для небольшого размера выборки (30 и менее).

#### Тест Шапиро и Уилка
Один из первых формальных тестов, который позволил обнаружить отклонения от нормальности из-за асимметрии, эксцесса, либо того и другого. Стал популярен, так как оказался мощным. Исходная статистика теста определяется как:
$$W = \frac{(\sum_{i=1}^na_iy_i)^2}{\sum_{i=1}^n(y_i-\bar y)^2}$$
$$a_i = (a_1, ..., a_n) = \frac{m^TV^{-1}}{(m^TV^{-1}V^{-1}m)^1/2}$$
Здесь $y_1 < y_2 < ... < y_n$ - упорядоченная выборка, $\bar y$ - выборочное среднее; $m = (m_1, ..., m_t)^T$ - ожидаемые значения порядковой статистики независимых и одинаково распределённых случайных величин, выбранных из стандартного нормального распределения, а $V$ - ковариационная матрица этой порядковой статистики.
Ограничение на размер выборки: $n < 50$
$0 \le W \le 1$:
Малые значения $W =$ отклонение гипотезы о нормальности;
$W$ близкие к единице указывает на нормальность данных.

#### Тест Шапиро и Уилка \ Модификации для больших выборок
SW-тест был модифицирован Ройстоном, чтобы увеличить ограничение размера выборки до 2000, после чего был предложен алгоритм AS181.
Позже Ройстон заметил, что аппроксимация Шапиро-Уилка для весов $a$, используемых в алгоритмах, неадекватна для $n > 50$.
Затем он дал улучшенную аппроксимацию весов и предложил алгоритм ASR94, который можно использовать для любого $n$ в диапазоне: $3 \le n \le 5000$

#### Стандартизация. Правило 2 и 3 $\sigma$
Пусть $x$ - распределена нормально, $\mu$ - математическое ожидание $x$, $\sigma$ - стандартное отклонение:
$x \approx N(\mu, \sigma)$
$\mu \pm \sigma \approx 68\%$
$\mu \pm 2\sigma \approx 95\%$
$\mu \pm 3\sigma \approx 99\%$

Какова вероятность получить $x > 154$, если $\mu = 150,\sigma=8$?
Используем стандартизацию:
$$z = \frac{154 - 150}8 = 0.5$$
Таблица $z$-распределения:
![[ЭТВМС(продолжение)z_распределение_таблица.png]]

Задачи:
1. Допустим, что некоторый признак распределён нормально, выборочное среднее = 100, дисперсия = 25 (M = 100, D = 25). Тогда приблизительно 95% всех наблюдений находится в диапазоне:
![[ЭТВМС(продолжение)_задача1_нормальное_распределение.png]]
2. Считается, что значение IQ у людей имеет нормальное распределение со средним значением, равным 100 и стандартным отклонением, равным 15 (M = 100, sd = 15). Какой приблизительный процент людей обладает IQ на промежутке от 70 до 112?
3. От 90 до 110 = 77% людей.
![[ЭТВМС(продолжение)_задача2_нормальное_распределение.png]]

#### Выборочное среднее
Рассчитайте стандартную ошибку среднего, если выборочное среднее 10, дисперсия 4, при N = 100.
Как соотносятся стандартная ошибка среднего и выборочное стандартное отклонение исследуемого признака (при размере выборки $n > 1$)?
$95\%$-ый доверительный интервал
$\bar x = 100, sd=5, n=100$;
$se = \frac{sd}{\sqrt n} = 0.5$
$\bar x \pm 1.96 \cdot 0.5$
![[ЭТВМС(продолжение)_задача3.png]]

#### Построение доверительных интервалов
Зададим малое число $0 < \alpha < 1$, например $\alpha=0.05$
интервал с концами $(\theta1; \theta2)$ называется доверительным интервалом для параметра $\theta$ с уровнем доверия $\gamma = 1 - \alpha$, если для любого $\theta \in \Theta$
$$Pr(\theta1 < \theta < \theta2) \ge 1 - \alpha$$
Если есть много повторных выборок, то д. и. можно найти как $\frac\alpha2$ и $(1 - \frac\alpha2$) - квантили вариационного ряда.
Например, для $n=1000$, 25-ое и 975-ое значения - концы $95\%$ д. и.
Поскольку таких повторностей практически не бывает, используют предположения о распределении и строят теоретический д. и.

#### Доверительный интервал для среднего. Выборка из нормального распределения
Пусть $x_1, ..., x_n$ - случайная выборка из нормально распределённой ГС.
Величина $\sqrt n \frac{\bar x - \mu}{\sigma} \approx N(0, 1) \rightarrow (\bar x \mp \frac{\sigma}{\sqrt n} z_\alpha)$
По теореме Фишера (1925 г.):
Выборочное среднее $\bar x$ и выборочная дисперсия $s^2$ независимы:
$\frac {(n-1)s^2}{\sigma^2}$ имеет распределение $\chi^2$ с $(n - 1)$ степенями свободы;
При неизвестной дисперсии:
$\sqrt n (\frac{\bar x - \mu}{s}) \approx T(n - 1)$ - распределение Стьюдента $(n - 1)$ степени свобод:
$$(\bar x - \frac s {\sqrt n} \cdot t_\alpha (n-1), (\bar x + \frac s {\sqrt n} \cdot t_\alpha (n-1))$$
Здесь $t_\alpha (n-1$) - квантиль распределения Стьюдента уровня $1 - \alpha / 2$ с $(n-1)$ степенью свободы.

Пусть $x_1, ..., x_n$ - случайная выборка из нормально распределённой ГС.
Величина $\sqrt n \frac{\bar x - \mu}{\sigma} \approx N(0, 1)$.
По теореме Фишера (1925 г.):
Выборочное среднее $\bar x$ и выборочная дисперсия $s^2$ независимы:
$\frac {(n-1)s^2}{\sigma^2}$ имеет распределение $\chi^2$ с $(n - 1)$ степенями свободы;
Доверительный интервал для дисперсии:
$$(\frac{(n-1)\cdot s^2}{\chi ^2 (1- \frac \alpha 2, n-1)}; \frac{(n-1)\cdot s^2}{\chi ^2 (\frac \alpha 2, n-1)})$$
Здесь $\chi ^2 (1 - \frac \alpha 2, n-1)$ и $\chi ^2 (\frac \alpha 2, n - 1)$ - соответствующие квантили распределения хи-квадрат с ($n - 1$) степенью свободы.
#### Факты
Распределение Стьюдента стремится к нормальному распределению при $n \rightarrow \infty$, поэтому при больших выборках доверительные интервалы для среднего, посчитанные по любой из формул, будут почти совпадать.
Распределение Стьюдента было введено в 1908 году Уильямом Сили Госсетом - ирландским служащим пивоваренного завода, который участвовал в разработке новых технологий производства пива и студентов не был. Опубликовать результаты исследований означало открыть корпоративную тайну. Поэтому Госсет печатал свои материалы под псевдонимом Стьюдент.
Фишер ввёл для распределения, открытого "Стьюдентом" обозначение t-распределение.

#### Без предположения о нормальном распределении...
Когда нет уверенности, что распределение нормальное (или хотя бы известное теоретические), или выборка небольшая, то применяют непараметрические методы, основанные на идеях Монте-Карло:
Непараметрические методы не зависят от какого-либо теоретического распределения и не используют его свойства;
Проверяют гипотезы, основываясь на свойствах эмпирических распределений, построенных на базе "псевдовыборок";
Получают оценки искомого параметра по "псевдовыборкам": мат. ожидание, дисперсию, д. и.

#### Непараметрические методы, основанные на идеях Монте-Карло
1. Метод складного ножа (jackknife)
2. Бутстреп (Bootstrap)
3. Перестановочный тест (permutation test)
4. Перекрёстные проверки (кросс-проверки, cross-validation)
Параметрические и непараметрические методы могут дополнять друг друга.

#### Непараметрические методы. Метод складного ножа (jackknife)
Предложен Морисом Кенуем в 1949 г.. Джон Тьюки расширил технику в 1958 г. и предложил название "складной нож". Это грубый инструмент, который может импровизировать решение для множества проблем, хотя конкретные проблемы могут быть более эффективно решены с помощью специально разработанного инструмента.
Складной нож является линейной аппроксимацией бутстрепа. 
Алгоритм:
Подмножество выборки - это также выборка из генеральной совокупности, поэтому
1. Создаём новые выборки, убирая по 1-му элементу из исходной выборки;
2. Получаем $n$ выборок, по которым рассчитываем оценки искомого параметра: среднее, дисперсию и т.д.
Особенно полезен для оценки смещения и дисперсии.
![[ЭТВМС(продолжение)_jackknife_схема.png]]

#### Метод складного ножа. Оценка среднего
Пусть оцениваемый параметр - это среднее значение случайной величины $x$, тогда для заданного набора независимых распределённых наблюдений $x_1, ..., x_n$, естественной оценкой является среднее значение выборки:
$$\bar x = \frac 1 n \sum_{i=1}^n x_i$$
Далее для каждого $i \in [n]$ рассчитываем среднее $\bar x_i$ подвыборки, состоящей из всех элементов, кроме $i$-го (назовём $i$-ой подвыборкой).
$$\bar x_{(i)} = \frac{1}{n-1} \sum_{j\in[n], j \ne i} x_j, i=1, ..., n$$
Можно предположить, что эти n подвыборок $\bar x_{(1)}, ..., \bar x_{(n)}$ дают аппроксимацию распределения выборочного среднего, и чем больше $n$, тем лучше эта аппроксимация (приближение).
Далее, чтобы получить оценку среднего методом jackknife, рассчитываем среднее по всем $n$ подвыборкам:
$$\bar x_{jack} = \frac 1 n \sum_{i=1}^n \bar x_{(i)}$$
Следующие вопросы к оценке среднего $\bar x_{jack}$:
Является ли она несмещённой? - не смещена, просто показать, что совпадает с обычным средним: $\frac 1 n \sum_{i = 1}^n \bar x _{(i)} = \bar x$
С дисперсией сложнее, т.к. повторности не являются независимыми.

Рассмотрели простой пример - оценку среднего значения - лишь для иллюстрации работы алгоритма. Настоящая сила этого метода проявляется, когда с его помощью оценивают другие параметры, такие как: моменты более высоких порядков, чем среднее значение, или другие функционалы распределения.

#### Расчёт смещения оценки
Метод складного ножа можно использовать для выявления (и исправления) смещения оценки, рассчитанного по всей выборке.
Пусть $\theta$ - параметр, который нужно оценить. Построим его оценку $\hat \theta = f_n(x_1, ..., x_n)$ на основе ограниченного набора наблюдений случайной величины $x:x_1, ..., x_n$
Оценка параметра $\hat \theta$ - случайная величина (меняется от выборки к выборке).
Пусть эта оценка смешена: $E_n(\hat \theta) = \theta + \frac \alpha n + \frac \beta {n^2} + ...$
$\hat \theta (i)$ - оценка параметра на подвыборке без $x_i$
Тогда $E(\hat \theta (i)) = \theta + \frac \alpha {n-1} + \frac \beta {(n-1)^2} + ...$
Найдём мат. ожидание средней оценки по методу "Складного ножа":
$E(\hat \theta _{jack}) = E(\sum_{i=1}^n \hat \theta (i) / n) = \frac 1 n \sum_{i=1}^n E(\hat \theta (i))$
$E (\hat \theta _{jack} = \frac 1 n \sum_{i = 1}^n (\theta + \frac \alpha {n - 1} + \frac \beta {(n-1)^2} + ...) = \theta + \frac \alpha {n-1} + \frac \beta {(n-1)^2} + ...$
$E(\hat \theta _{jack} - E_n(\hat \theta) = \theta + \frac \alpha {n-1} + \frac \beta {(n-1)^2} + ... - \theta - \frac \alpha n - \frac \beta {n^2} - ... \approx \frac \alpha {n(n-1)}$
Далее найдём оценку смещения складного ножа:
$\frac \alpha n \approx (n-1) \cdot (E(\hat \theta _{jack}) - E_n(\hat \theta))$
$\hat {bias}_{jack} = (n - 1) \cdot (\hat \theta _{jack} - \hat \theta)$
И окончательно получаем оценку с коррекцией смещения:
$\hat \theta _{jack}^* = \hat \theta - \hat {bias}_{jack} = n \hat \theta - (n-1) \hat \theta _{jack}$

#### Непараметрические методы. Бутстреп (Bootstrap)
Предложил Brad Efron (1979), идея метода:
Из генеральной совокупности формируется случайная выборка из $N(t)$ наблюдений. Из выборки $N(t)$ набирается случайная выборка с возвратом (псевдовыборка) того же объёма, но в которую некоторые наблюдения могут попасть несколько раз, а другие не попасть совсем. Процедура перевыборки повторяется достаточно много раз (обычно тысячи).
По каждой из псевдовыборок оценивается искомый параметр (среднее, дисперсия) и строится его эмпирическое распределение. Далее можно строить д. и. на основе процентилей полученного распределения.

#### К вопросу интерпретации результатов графических тестов на нормальность
![[ЭТВМС(продолжение)_интерпретация_графических_тестов_на_нормальность.png]]
QQ plot - из-за ошибок выборочности будет отклонение от теоретического. Как отличить допустимое от недопустимого отклонения? Параметрический бутстреп:
Определим параметры распределения по выборке, построим бутстрепный д. и., набрав псевдовыборки из н. р. с найденными параметрами и того же объёма, что и в эксперименты.

Гистограмма распределения, пример:
```
set.seed(337)
y<-norm(20, mean=0, sd=1)
```
![[ЭТВМС(продолжение)_имитированная_выборка.png]]
![[ЭТВМС(продолжение)_Колмогоров_Смирнов_тест_гистограмма.png]]

#### Ссылки
1. [Лекции по математическим методам анализа данных 2025]( https://cloud.mail.ru/public/T6ge/9337oqgK3)